{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f037c8",
   "metadata": {},
   "source": [
    "# Notebook Prétraitement Prédiction\n",
    "\n",
    "L'objectif de ce notebook est de construire un modèle de scoring pouvant renvoyer une prédiction sur la probabilité de faillite d'un client donné.\n",
    "\n",
    "* [Partie 1 : import des packages et déclarations des fonctions, constantes et variables](#partie1)\n",
    "    * [Import des packages](#section_1_1)\n",
    "    * [Décorateur de mesure du temps d'éxecution](#section_1_2)\n",
    "    * [Fonctions liées au scoring](#section_1_3)\n",
    "    * [Fonction de report des statistiques principales](#section_1_4)\n",
    "    * [Fonctions liées à la jointure des fichiers .csv](#section_1_5)\n",
    "    * [Fonctions liées à la préparation des datasets pour la modélisation](#section_1_6)\n",
    "    * [Déclaration des constantes / variables principales](#section_1_7)\n",
    "    * [Déclaration des espaces de recherche](#section_1_8)\n",
    "\n",
    "* [Partie 2 : Pré-traitement du dataset](#partie2)\n",
    "    * [Extraction des fichiers de l'archive](#section_2_1)\n",
    "    * [Pré-traitement initial : fusion des .csv / conservation des variables principales](#section_2_2)\n",
    "    * [Evaluation de la quantité de valeurs manquantes](#section_2_3)\n",
    "    * [Imputation des valeurs manquantes](#section_2_4)\n",
    "    * [Feature Engineering](#section_2_5)\n",
    "    * [Visualisations après prétraitement](#section_2_6)\n",
    "    * [Exportation du jeu de données total après prétraitement](#section_2_7)\n",
    "\n",
    "* [Partie 3 : Modélisation](#partie3)\n",
    "    * [Initialisation de MLFlow](#section_3_1)\n",
    "    * [Préparation des datasets d'entraînement / test / score](#section_3_2)\n",
    "    * [Modèle Baseline : Dummy Classifier](#section_3_3)\n",
    "    * [Régression Logistique](#section_3_4)\n",
    "    * [XGBoost](#section_3_5)\n",
    "    * [LightGBM](#section_3_6)\n",
    "    * [Évaluation de la courbe ROC (AUC)](#section_3_7)\n",
    "    * [Fine Tuning du modèle sélectionné](#section_3_8)\n",
    "    * [Évaluation de la courbe ROC (AUC) du modèle Fine Tuned](#section_3_9)\n",
    "    * [Déclaration du modèle final](#section_3_10)\n",
    "    * [Création des SHAP Explainer / Values pour l'interprétabilité locale et globale](#section_3_11)\n",
    "    * [Sérialisation des modèles et des SHAP Explainer / Values](#section_3_12)\n",
    "    * [Chargement des datasets sauvegardés précédemment et définition des datasets évalués](#section_3_13)\n",
    "\n",
    "* [Partie 4 : Analyse du Data Drift (evidently)](#partie4)\n",
    "    * [Train vs Test](#section_4_1)\n",
    "    * [Train vs Score](#section_4_2)\n",
    "    * [Train vs App](#section_4_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc34c64",
   "metadata": {},
   "source": [
    "# Partie 1 : import des packages et déclarations des fonctions, constantes et variables<a class=\"anchor\" id=\"partie1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce26ca",
   "metadata": {},
   "source": [
    "### Import des packages<a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1048cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45babb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np   \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, auc, RocCurveDisplay, make_scorer, fbeta_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics.base_metric import generate_column_metrics\n",
    "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n",
    "from evidently.metrics import *\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests.base_test import generate_column_tests\n",
    "from evidently.test_preset import DataStabilityTestPreset, NoTargetPerformanceTestPreset\n",
    "from evidently.tests import *\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b91e17",
   "metadata": {},
   "source": [
    "### Décorateur de mesure du temps d'éxecution<a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31996340",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - réalisé en {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608e9a8",
   "metadata": {},
   "source": [
    "### Fonctions liées au scoring<a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(y, t):\n",
    "    out = []\n",
    "    for pred in (y):\n",
    "        if (pred > t):\n",
    "            out.append(0)\n",
    "        else:\n",
    "            out.append(1)\n",
    "    return np.array(out)\n",
    "\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "\n",
    "def select_best_threshold(true_y, pred_y):\n",
    "    # define thresholds\n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "    # evaluate each threshold\n",
    "    scores = [fbeta_score(true_y, to_labels(pred_y, t), beta=2) for t in thresholds]\n",
    "    # return best threshold\n",
    "    return thresholds[np.argmax(scores)]\n",
    "\n",
    "def business_cost(true_y, pred_y):\n",
    "    score = 0\n",
    "    mismatch_fp = 1\n",
    "    mismatch_fn = mismatch_fp * 10\n",
    "    \n",
    "    true_y = true_y.tolist()\n",
    "    pred_y = to_labels(pred_y, select_best_threshold(true_y, pred_y))\n",
    "    \n",
    "    for i in range(len(true_y)):\n",
    "        if (true_y[i] != pred_y[i]):\n",
    "            if (true_y[i] == 1):\n",
    "                score += mismatch_fp\n",
    "            else:\n",
    "                score += mismatch_fn\n",
    "\n",
    "    return score\n",
    "\n",
    "def fbeta_cost(true_y, pred_y):\n",
    "    true_y = true_y.tolist()\n",
    "    pred_y = to_labels(pred_y, select_best_threshold(true_y, pred_y))\n",
    "    return fbeta_score(true_y, pred_y, beta=2, average='binary', pos_label=0)\n",
    "\n",
    "business_loss = make_scorer(business_cost, greater_is_better=False, needs_proba=True)\n",
    "fbeta_loss = make_scorer(fbeta_cost, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66430039",
   "metadata": {},
   "source": [
    "### Fonction de report des statistiques principales<a class=\"anchor\" id=\"section_1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f93a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    # Recherche du meilleur seuil de décision sur le jeu d'entraînement\n",
    "    y_proba = model.predict_proba(X_train)\n",
    "    probs = y_proba[:, 1]\n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "    scores = [fbeta_score(y_train, to_labels(probs, t), beta=2) for t in thresholds]\n",
    "    ix = np.argmax(scores)\n",
    "\n",
    "    # Exploitation du seuil et évaluation sur jeu de test\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    probs = y_proba[:, 1]\n",
    "    y_pred = to_labels(probs, thresholds[ix])\n",
    "    \n",
    "    business_cost_model = business_cost(y_test, y_pred)\n",
    "    model_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Enregistrement dans MLFlow\n",
    "\n",
    "    mlflow.log_metric('Best threshold', thresholds[ix])\n",
    "    mlflow.log_metric('Best FBeta Score', scores[ix])\n",
    "    mlflow.log_metric('Business Scoring value', business_cost_model)\n",
    "\n",
    "    mlflow.log_metric('Negative targets Precision', str(round(model_report['0']['precision'],2)))\n",
    "    mlflow.log_metric('Negative targets Recall', str(round(model_report['0']['recall'],2)))\n",
    "    mlflow.log_metric('Negative targets F1-Score', str(round(model_report['0']['f1-score'],2)))\n",
    "    mlflow.log_metric('Positive targets Precision', str(round(model_report['1']['precision'],2)))\n",
    "    mlflow.log_metric('Positive targets Recall', str(round(model_report['1']['recall'],2)))\n",
    "    mlflow.log_metric('Positive targets F1-Score', str(round(model_report['1']['f1-score'],2)))\n",
    "\n",
    "    mlflow.log_metric('Accuracy', str(round(model_report['accuracy'],2)))\n",
    "\n",
    "    mlflow.log_metric('Macro AVG Precision', str(round(model_report['macro avg']['precision'],2)))\n",
    "    mlflow.log_metric('Macro AVG Recall', str(round(model_report['macro avg']['recall'],2)))\n",
    "    mlflow.log_metric('Macro AVG F1-Score', str(round(model_report['macro avg']['f1-score'],2)))\n",
    "    mlflow.log_metric('Weighted AVG Precision', str(round(model_report['weighted avg']['precision'],2)))\n",
    "    mlflow.log_metric('Weighted AVG Recall', str(round(model_report['weighted avg']['recall'],2)))\n",
    "    mlflow.log_metric('Weighted AVG F1-Score', str(round(model_report['weighted avg']['f1-score'],2)))\n",
    "    \n",
    "    # Actuellement davantage un contournement, pas très production friendly mais fonctionnel\n",
    "    if (model_name not in [\"lightgbm\", \"dummy\"]): # Modèles posant problème avec SHAP\n",
    "        if (model_name == 'logistic_regression'):\n",
    "            shap_explainer = shap.Explainer(model, X_test, algorithm = 'auto', n_jobs = -1)\n",
    "            shap_values = shap_explainer(X_test)\n",
    "        else:\n",
    "            shap_explainer = shap.TreeExplainer(model, X_test, algorithm = 'auto', n_jobs = -1)\n",
    "            shap_values = shap_explainer(X_test)\n",
    "\n",
    "        plt.clf()\n",
    "        shap.plots.beeswarm(shap_values, show=False)\n",
    "        plt.savefig(model_name + '_shap_beeswarm.png', bbox_inches='tight', dpi=300)\n",
    "        beeswarm, ax = plt.subplots()\n",
    "        ax.imshow(plt.imread(model_name + '_shap_beeswarm.png'))\n",
    "        mlflow.log_figure(beeswarm, model_name + '_shap_beeswarm.png')\n",
    "\n",
    "        plt.clf()\n",
    "        shap.plots.bar(shap_values, show=False)\n",
    "        plt.savefig(model_name + '_shap_bar.png', bbox_inches='tight', dpi=300)\n",
    "        bar, ax = plt.subplots()\n",
    "        ax.imshow(plt.imread(model_name + '_shap_bar.png'))\n",
    "        mlflow.log_figure(bar, model_name + '_shap_bar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0361d",
   "metadata": {},
   "source": [
    "### Fonctions liées à la jointure des fichiers .csv<a class=\"anchor\" id=\"section_1_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploitation de pd.get_dummies pour le one-hot encoding de variables catégorielles\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess de application_train.csv et application_test.csv\n",
    "def application_train_test(base_path, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv(base_path + 'application_train.csv')\n",
    "    test_df = pd.read_csv(base_path + 'application_test.csv')\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = pd.concat([df, test_df], ignore_index=True).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(base_path, nan_as_category = True):\n",
    "    bureau = pd.read_csv(base_path + 'bureau.csv')\n",
    "    bb = pd.read_csv(base_path + 'bureau_balance.csv')\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n",
    "\n",
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(base_path, nan_as_category = True):\n",
    "    prev = pd.read_csv(base_path + 'previous_application.csv')\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(base_path, nan_as_category = True):\n",
    "    pos = pd.read_csv(base_path + 'POS_CASH_balance.csv')\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(base_path, nan_as_category = True):\n",
    "    ins = pd.read_csv(base_path + 'installments_payments.csv')\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(base_path, nan_as_category = True):\n",
    "    cc = pd.read_csv(base_path + 'credit_card_balance.csv')\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg\n",
    "\n",
    "def big_merge(base_path):\n",
    "    \"\"\" Je vais fusionner toutes les colonnes présentes \"\"\"\n",
    "    df = application_train_test(base_path)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(base_path)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(base_path)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(base_path)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(base_path)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(base_path)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "        \n",
    "    return df\n",
    "    \n",
    "def big_drop(input_df, features):\n",
    "    \"\"\" Je renvoie le dataframe d'entrée avec les colonnes indiquées par features \"\"\"\n",
    "    output_df = input_df[features]\n",
    "    return output_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca965ec4",
   "metadata": {},
   "source": [
    "### Fonctions liées à la préparation des datasets pour la modélisation<a class=\"anchor\" id=\"section_1_6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24996a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets(train_df, test_df, score_df):\n",
    "    train_df.to_csv(OUTPUT_DATASETS_PATH + 'train_data.csv', header=True, index=False)\n",
    "    test_df.to_csv(OUTPUT_DATASETS_PATH + 'test_data.csv', header=True, index=False)\n",
    "    score_df.to_csv(OUTPUT_DATASETS_PATH + 'score_data.csv', header=True, index=False)\n",
    "\n",
    "def dataset_splitter(input_df, splitter, features, target):\n",
    "    for train_index, test_index in splitter.split(input_df[features], input_df[target]):\n",
    "        dataset_1 = input_df.loc[test_index]\n",
    "        dataset_2 = input_df.loc[train_index]\n",
    "    return dataset_1, dataset_2\n",
    "\n",
    "def prepare_datasets(input_df):    \n",
    "    # Préparation des variables principales\n",
    "    # Features\n",
    "    features = [f for f in input_df.columns if f not in ['level_0', 'TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    # Cible\n",
    "    target = 'TARGET'\n",
    "    # Méthodes de découpe du dataset initial en jeux d'entraînement, test et score\n",
    "    score_spliter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=111)\n",
    "    train_spliter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    # Création de train_df, test_df, score_df\n",
    "    score_df, main_df = dataset_splitter(input_df, score_spliter, features, target)\n",
    "    # Reset des index de main_df\n",
    "    main_df.reset_index(inplace=True)\n",
    "    test_df, train_df = dataset_splitter(main_df, train_spliter, features, target)\n",
    "    # Elimination des colonnes indésirables, reset final des index\n",
    "    train_df.drop(columns=['level_0'], axis=1, inplace=True)\n",
    "    test_df.drop(columns=['level_0'], axis=1, inplace=True)\n",
    "    train_df.reset_index(inplace=True)\n",
    "    test_df.reset_index(inplace=True)\n",
    "    score_df.reset_index(inplace=True)\n",
    "    # Export des datasets pour emploi ultérieur\n",
    "    export_datasets(train_df, test_df, score_df)\n",
    "    # Elimination des dataframes initiaux\n",
    "    del score_df\n",
    "    del input_df\n",
    "    gc.collect()\n",
    "    # Création des sorties\n",
    "    X_train = train_df[features]\n",
    "    X_test = test_df[features]\n",
    "    y_train = train_df[target]\n",
    "    y_test = test_df[target]\n",
    "    # Sortie\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ef591",
   "metadata": {},
   "source": [
    "### Déclaration des constantes / variables principales<a class=\"anchor\" id=\"section_1_7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLED_MODELS_PATH = \"models/\"\n",
    "INPUT_DATA_PATH = \"input_data/\"\n",
    "OUTPUT_DATASETS_PATH = \"output_data/\"\n",
    "EVIDENTLY_PATH = \"data_drift/\"\n",
    "\n",
    "# Cross validation\n",
    "n_folds = 10\n",
    "cv = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=2, random_state=42)\n",
    "# Méthodes de scoring dans la cross validation\n",
    "scoring = {'business_loss' : business_loss, 'fbeta_loss' : fbeta_loss}\n",
    "\n",
    "os.mkdir(PICKLED_MODELS_PATH) if (os.path.exists(PICKLED_MODELS_PATH) == False) else None\n",
    "os.mkdir(INPUT_DATA_PATH) if (os.path.exists(INPUT_DATA_PATH) == False) else None\n",
    "os.mkdir(OUTPUT_DATASETS_PATH) if (os.path.exists(OUTPUT_DATASETS_PATH) == False) else None\n",
    "os.mkdir(EVIDENTLY_PATH) if (os.path.exists(EVIDENTLY_PATH) == False) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0f1a2",
   "metadata": {},
   "source": [
    "### Déclaration des espaces de recherche<a class=\"anchor\" id=\"section_1_8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'espace de recherche de la Régression Logistique pour la RandomSearchCV\n",
    "lr_space = dict()\n",
    "lr_space['lr__C'] = np.logspace(-3, 3, 7)\n",
    "lr_space['lr__penalty'] = ['l2']\n",
    "lr_space['lr__solver'] = ['liblinear', 'newton-cg', 'lbfgs']\n",
    "lr_space['lr__class_weight'] = [{ 0:0.95, 1:0.05 }, { 0:0.55, 1:0.45 }, { 0:0.45, 1:0.55 },{ 0:0.05, 1:0.95 }]\n",
    "\n",
    "# Définition de l'espace de recherche du XGBoost pour la RandomSearchCV\n",
    "xgboost_space = dict()\n",
    "xgboost_space['xgb__n_estimators'] = [1000, 3000, 6000]\n",
    "xgboost_space['xgb__gamma'] = [1, 4, 7, 9]\n",
    "xgboost_space['xgb__colsample_bytree'] = [0.75, 0.95]\n",
    "xgboost_space['xgb__subsample'] = [1, 9]\n",
    "xgboost_space['xgb__max_depth'] = [2, 3]\n",
    "xgboost_space['xgb__reg_alpha'] = [0.02, 0.03, 0.04]\n",
    "xgboost_space['xgb__reg_lambda'] = [0.05, 0.06, 0.07]\n",
    "xgboost_space['xgb__min_child_weight'] = [500, 1000, 2500, 5000, 10000]\n",
    "xgboost_space['xgb__tree_method'] = ['gpu_hist']\n",
    "\n",
    "# Définition de l'espace de recherche du LightGBM pour la RandomSearchCV\n",
    "lightgbm_space = dict()\n",
    "lightgbm_space['lgbm__n_estimators'] = [1000, 3000, 6000, 10000]\n",
    "lightgbm_space['lgbm__learning_rate'] = [0.01, 0.02]\n",
    "lightgbm_space['lgbm__num_leaves'] = [30, 35]\n",
    "lightgbm_space['lgbm__colsample_bytree'] = [0.75, 0.95]\n",
    "lightgbm_space['lgbm__subsample'] = [0.75, 0.90]\n",
    "lightgbm_space['lgbm__max_depth'] = [2, 4, 6]\n",
    "lightgbm_space['lgbm__reg_alpha'] = [0.02, 0.04]\n",
    "lightgbm_space['lgbm__reg_lambda'] = [0.05, 0.07]\n",
    "lightgbm_space['lgbm__min_split_gain'] = [0.01, 0.02]\n",
    "lightgbm_space['lgbm__bagging_fraction'] = [0.8]\n",
    "lightgbm_space['lgbm__bagging_freq'] = [10]\n",
    "lightgbm_space['lgbm__feature_fraction'] = [0.8]\n",
    "lightgbm_space['lgbm__min_data_in_leaf'] = [18000, 20000, 22000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3a56f",
   "metadata": {},
   "source": [
    "# Partie 2 : Pré-traitement du dataset<a class=\"anchor\" id=\"partie2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c77bb",
   "metadata": {},
   "source": [
    "### Extraction des fichiers de l'archive<a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f127cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_files = [\"application_test.csv\",\n",
    "                  \"application_train.csv\",\n",
    "                  \"bureau.csv\",\n",
    "                  \"bureau_balance.csv\",\n",
    "                  \"credit_card_balance.csv\",\n",
    "                  \"HomeCredit_columns_description.csv\",\n",
    "                  \"installments_payments.csv\",\n",
    "                  \"POS_CASH_balance.csv\",\n",
    "                  \"previous_application.csv\",\n",
    "                  \"sample_submission.csv\"]\n",
    "    if all([os.path.isfile(INPUT_DATA_PATH + cur_file) for cur_file in input_files]):\n",
    "        break\n",
    "    else:\n",
    "        with ZipFile(\"home-credit-default-risk.zip\", \"r\") as zObject:\n",
    "                zObject.extractall(path=INPUT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a54e2",
   "metadata": {},
   "source": [
    "### Pré-traitement initial : fusion des .csv / conservation des variables principales<a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc891142",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_list = ['AMT_ANNUITY',\n",
    "                           'AMT_CREDIT',\n",
    "                           'AMT_GOODS_PRICE',\n",
    "                           'BURO_AMT_CREDIT_MAX_OVERDUE_MEAN',\n",
    "                           'BURO_AMT_CREDIT_SUM_MEAN',\n",
    "                           'BURO_DAYS_CREDIT_ENDDATE_MAX',\n",
    "                           'BURO_DAYS_CREDIT_MAX',\n",
    "                           'DAYS_BIRTH',\n",
    "                           'DAYS_EMPLOYED',\n",
    "                           'DAYS_ID_PUBLISH',\n",
    "                           'DAYS_LAST_PHONE_CHANGE',\n",
    "                           'DAYS_REGISTRATION',\n",
    "                           'EXT_SOURCE_1',\n",
    "                           'EXT_SOURCE_2',\n",
    "                           'EXT_SOURCE_3',\n",
    "                           'OWN_CAR_AGE']\n",
    "\n",
    "non_features = ['SK_ID_CURR',\n",
    "                'TARGET']\n",
    "\n",
    "features = feature_importance_list\n",
    "features_target = features + non_features\n",
    "\n",
    "merged_df = big_merge(INPUT_DATA_PATH)\n",
    "\n",
    "print(\"Taux de valeurs manquantes initial : \", str(merged_df.isna().mean().mean()))\n",
    "\n",
    "trimmed_df = big_drop(merged_df, features_target)\n",
    "trimmed_df.drop(columns=['index'], axis=1, inplace=True)\n",
    "trimmed_df.reset_index(inplace=True)\n",
    "\n",
    "del merged_df\n",
    "gc.collect()\n",
    "\n",
    "newclients_df = trimmed_df[trimmed_df.loc[:, 'TARGET'].isna() == True]\n",
    "newclients_df.drop(columns=['index', 'TARGET'], axis=1, inplace=True)\n",
    "newclients_df.reset_index(inplace=True)\n",
    "\n",
    "mask = trimmed_df[trimmed_df.loc[:, 'TARGET'].isna() == True].index\n",
    "trimmed_df.drop(index=mask, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b496a60",
   "metadata": {},
   "source": [
    "### Evaluation de la quantité de valeurs manquantes<a class=\"anchor\" id=\"section_2_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec819a",
   "metadata": {},
   "source": [
    "Hormis EXT_SOURCE_1 et OWN_CAR_AGE, la quantité de valeurs manquantes reste gérable. Je peux me permettre de créer un dataframe se séparant de toutes les lignes comportant des valeurs manquantes et évaluer les corrélations afin d'envisager de l'iterative imputing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcbbf2",
   "metadata": {},
   "source": [
    "### Imputation des valeurs manquantes<a class=\"anchor\" id=\"section_2_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = trimmed_df.dropna(how='any')\n",
    "\n",
    "correlated_columns = set()\n",
    "\n",
    "lim_max = 0.5\n",
    "lim_min = -0.5\n",
    "\n",
    "for var_1 in (trimmed_df.columns):\n",
    "    for var_2 in (trimmed_df.columns):\n",
    "        if (var_1 != var_2 and var_1 not in non_features and var_2 not in non_features):\n",
    "            try:\n",
    "                result = pearsonr(temp_df[var_1], temp_df[var_2])[0]\n",
    "                if (result > lim_max or result < lim_min):\n",
    "                    output = sorted([var_1, var_2])\n",
    "                    correlated_columns.add(tuple(output))\n",
    "            except:\n",
    "                continue\n",
    "correlated_columns = list(correlated_columns)\n",
    "\n",
    "for cor_columns in (correlated_columns):\n",
    "    imputer = IterativeImputer(estimator=BayesianRidge())\n",
    "    imputed = imputer.fit_transform(trimmed_df[list(cor_columns)])\n",
    "    trimmed_df[list(cor_columns)] = pd.DataFrame(imputed, columns=trimmed_df[list(cor_columns)].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb884cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ccb800",
   "metadata": {},
   "source": [
    "Toutes les colonnes ne sont pas remplies. Je vais de suite remplacer les valeurs manquantes par une valeur extrême prise en compte dans les modèles les plus raffinés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87139123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation \"simple\" (efficace pour les XGBoost et LightGBM notamment)\n",
    "trimmed_df.replace(np.nan, -99999999, inplace=True)\n",
    "newclients_df.replace(np.nan, -99999999, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3437b12",
   "metadata": {},
   "source": [
    "### Feature Engineering<a class=\"anchor\" id=\"section_2_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b806c38",
   "metadata": {},
   "source": [
    "Basé sur différentes discussions sur la compétition Kaggle : https://www.kaggle.com/competitions/home-credit-default-risk/discussion  \n",
    "Certaines features semblent intéressantes à créer, basées sur 3 variables initiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d310934",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trimmed_df.columns) - 1 # Je retire index du décompte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMT_CREDIT_TO_ANNUITY_RATIO\n",
    "trimmed_df['AMT_CREDIT_TO_ANNUITY_RATIO'] = trimmed_df['AMT_CREDIT'] / trimmed_df['AMT_ANNUITY']\n",
    "newclients_df['AMT_CREDIT_TO_ANNUITY_RATIO'] = newclients_df['AMT_CREDIT'] / newclients_df['AMT_ANNUITY']\n",
    "# AMT_GOODS_PRICE_TO_ANNUITY_RATIO\n",
    "trimmed_df['AMT_GOODS_PRICE_TO_ANNUITY_RATIO'] = trimmed_df['AMT_GOODS_PRICE'] / trimmed_df['AMT_ANNUITY']\n",
    "newclients_df['AMT_GOODS_PRICE_TO_ANNUITY_RATIO'] = newclients_df['AMT_GOODS_PRICE'] / newclients_df['AMT_ANNUITY']\n",
    "# AMT_CREDIT_GOODS_PRICE_DIFF\n",
    "trimmed_df['AMT_CREDIT_GOODS_PRICE_DIFF'] = trimmed_df['AMT_CREDIT'] - trimmed_df['AMT_GOODS_PRICE']\n",
    "newclients_df['AMT_CREDIT_GOODS_PRICE_DIFF'] = newclients_df['AMT_CREDIT'] - newclients_df['AMT_GOODS_PRICE']\n",
    "# GOODS_PRICE_FAIL_RATIO\n",
    "trimmed_df['GOODS_PRICE_FAIL_RATIO'] = (trimmed_df['AMT_CREDIT'] - trimmed_df['AMT_GOODS_PRICE']) / trimmed_df['AMT_GOODS_PRICE']\n",
    "newclients_df['GOODS_PRICE_FAIL_RATIO'] = (newclients_df['AMT_CREDIT'] - newclients_df['AMT_GOODS_PRICE']) / newclients_df['AMT_GOODS_PRICE']\n",
    "# CREDIT_FAIL_RATIO\n",
    "trimmed_df['CREDIT_FAIL_RATIO'] = (trimmed_df['AMT_CREDIT'] - trimmed_df['AMT_GOODS_PRICE']) / trimmed_df['AMT_CREDIT']\n",
    "newclients_df['CREDIT_FAIL_RATIO'] = (newclients_df['AMT_CREDIT'] - newclients_df['AMT_GOODS_PRICE']) / newclients_df['AMT_CREDIT']\n",
    "\n",
    "trimmed_df.drop(columns=['AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE'], axis=1, inplace=True)\n",
    "newclients_df.drop(columns=['AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE'], axis=1, inplace=True)\n",
    "\n",
    "features_target.remove('AMT_ANNUITY')\n",
    "features_target.remove('AMT_CREDIT')\n",
    "features_target.remove('AMT_GOODS_PRICE')\n",
    "\n",
    "newclients_df.to_csv(OUTPUT_DATASETS_PATH + 'newclients_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f714a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4f264",
   "metadata": {},
   "source": [
    "### Visualisations après prétraitement<a class=\"anchor\" id=\"section_2_6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df.hist(figsize=(20,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "r = trimmed_df[features_target].corr(method=\"pearson\")\n",
    "mask = np.triu(np.ones_like(r, dtype=bool))\n",
    "plt.figure(figsize=(12,12))\n",
    "heatmap = sns.heatmap(trimmed_df[features_target].corr(), mask=mask, vmin=-1, vmax=1, annot=False, cmap='coolwarm', square=True, linewidths=1.0, cbar_kws={\"shrink\": .5})\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=-45, rotation_mode='anchor', ha='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a93f3",
   "metadata": {},
   "source": [
    "Je n'observe pas de corrélations aberrantes avec la sortie ('TARGET'), le data leaking est donc évité à ce niveau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac73bc",
   "metadata": {},
   "source": [
    "### Exportation du jeu de données total après prétraitement<a class=\"anchor\" id=\"section_2_7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df['TARGET'] = trimmed_df['TARGET'].astype('int')\n",
    "trimmed_df.to_csv(OUTPUT_DATASETS_PATH + 'main_dataframe_pretreated.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8f655",
   "metadata": {},
   "source": [
    "# Partie 3 : Modélisation<a class=\"anchor\" id=\"partie3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1948c",
   "metadata": {},
   "source": [
    "### Initialisation de MLFlow<a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8be5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.autolog()\n",
    "mlflow.sklearn.autolog()\n",
    "mlflow.lightgbm.autolog()\n",
    "mlflow.xgboost.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec839f",
   "metadata": {},
   "source": [
    "### Préparation des datasets d'entraînement / test / score<a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ab480",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_datasets(trimmed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a9af8",
   "metadata": {},
   "source": [
    "### Modèle Baseline : Dummy Classifier<a class=\"anchor\" id=\"section_3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786109da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    dummy = DummyClassifier(strategy = \"stratified\")\n",
    "    with timer(\"Entraînement du dummy model\"):\n",
    "        dummy.fit(X_train, y_train);\n",
    "    y_dummy = dummy.predict(y_train)\n",
    "    generate_stats(dummy, 'dummy', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebedf7",
   "metadata": {},
   "source": [
    "### Régression Logistique<a class=\"anchor\" id=\"section_3_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    lr_pipeline = imbpipeline([('smote', SMOTE(random_state=42)), ('lr', lr)])\n",
    "    \n",
    "    lr_grid = RandomizedSearchCV(estimator=lr_pipeline,\n",
    "                                 param_distributions=lr_space,\n",
    "                                 scoring=scoring,\n",
    "                                 refit='fbeta_loss',\n",
    "                                 n_iter=15,\n",
    "                                 cv=cv,\n",
    "                                 n_jobs=-1)\n",
    "    lr_grid.fit(X_train, y_train)\n",
    "\n",
    "    lr = LogisticRegression(C=lr_grid.best_params_['lr__C'],\n",
    "                            penalty=lr_grid.best_params_['lr__penalty'],\n",
    "                            solver=lr_grid.best_params_['lr__solver'],\n",
    "                            class_weight=lr_grid.best_params_['lr__class_weight'])\n",
    "    \n",
    "    with timer(\"Entraînement du logistic regression model\"):\n",
    "        lr.fit(X_train, y_train);\n",
    "    \n",
    "    generate_stats(lr, 'logistic_regression', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ceae60",
   "metadata": {},
   "source": [
    "### XGBoost<a class=\"anchor\" id=\"section_3_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbfb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    xgboost = XGBClassifier()\n",
    "    \n",
    "    xgboost_pipeline = imbpipeline([('smote', SMOTE(random_state=42)), ('xgb', xgboost)])\n",
    "    \n",
    "    xgboost_grid = RandomizedSearchCV(estimator=xgboost_pipeline,\n",
    "                                      param_distributions=xgboost_space,\n",
    "                                      scoring=scoring,\n",
    "                                      refit='fbeta_loss',\n",
    "                                      n_iter=15,\n",
    "                                      cv=cv,\n",
    "                                      n_jobs=-1)\n",
    "    xgboost_grid.fit(X_train, y_train)\n",
    "\n",
    "    xgboost = XGBClassifier(tree_method=xgboost_grid.best_params_['xgb__tree_method'],\n",
    "                            subsample=xgboost_grid.best_params_['xgb__subsample'],\n",
    "                            reg_lambda=xgboost_grid.best_params_['xgb__reg_lambda'],\n",
    "                            reg_alpha=xgboost_grid.best_params_['xgb__reg_alpha'],\n",
    "                            min_child_weight=xgboost_grid.best_params_['xgb__min_child_weight'],\n",
    "                            max_depth=xgboost_grid.best_params_['xgb__max_depth'],\n",
    "                            gamma=xgboost_grid.best_params_['xgb__gamma'],\n",
    "                            colsample_bytree=xgboost_grid.best_params_['xgb__colsample_bytree'],\n",
    "                            predictor='gpu_predictor')\n",
    "\n",
    "    with timer(\"Entraînement du XGBoost model\"):\n",
    "        xgboost.fit(X_train, y_train);\n",
    "    \n",
    "    generate_stats(xgboost, 'xgboost', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf292fe0",
   "metadata": {},
   "source": [
    "### LightGBM<a class=\"anchor\" id=\"section_3_6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    lgbm = LGBMClassifier()\n",
    "    \n",
    "    lgbm_pipeline = imbpipeline([('smote', SMOTE(random_state=42)), ('lgbm', lgbm)])\n",
    "    \n",
    "    lgbm_grid = RandomizedSearchCV(estimator=lgbm_pipeline,\n",
    "                                   param_distributions=lightgbm_space,\n",
    "                                   scoring=scoring,\n",
    "                                   refit='fbeta_loss',\n",
    "                                   n_iter=15,\n",
    "                                   cv=cv,\n",
    "                                   n_jobs=-1)\n",
    "    lgbm_grid.fit(X_train, y_train)\n",
    "\n",
    "    lgbm = LGBMClassifier(n_estimators=lgbm_grid.best_params_['lgbm__n_estimators'],\n",
    "                          learning_rate=lgbm_grid.best_params_['lgbm__learning_rate'],\n",
    "                          num_leaves=lgbm_grid.best_params_['lgbm__num_leaves'],\n",
    "                          colsample_bytree=lgbm_grid.best_params_['lgbm__colsample_bytree'],\n",
    "                          subsample=lgbm_grid.best_params_['lgbm__subsample'],\n",
    "                          max_depth=lgbm_grid.best_params_['lgbm__max_depth'],\n",
    "                          reg_alpha=lgbm_grid.best_params_['lgbm__reg_alpha'],\n",
    "                          reg_lambda=lgbm_grid.best_params_['lgbm__reg_lambda'],\n",
    "                          min_split_gain=lgbm_grid.best_params_['lgbm__min_split_gain'],\n",
    "                          bagging_fraction=lgbm_grid.best_params_['lgbm__bagging_fraction'],\n",
    "                          bagging_freq=lgbm_grid.best_params_['lgbm__bagging_freq'],\n",
    "                          feature_fraction=lgbm_grid.best_params_['lgbm__feature_fraction'],\n",
    "                          min_data_in_leaf=lgbm_grid.best_params_['lgbm__min_data_in_leaf'],\n",
    "                          device=\"cuda\")\n",
    "\n",
    "    with timer(\"Entraînement du LightGBM model\"):\n",
    "        lgbm.fit(X_train, y_train);\n",
    "\n",
    "    generate_stats(lgbm, 'lightgbm', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bf665",
   "metadata": {},
   "source": [
    "### Évaluation de la courbe ROC (AUC)<a class=\"anchor\" id=\"section_3_7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f91f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    xgboost, X_train, y_train, color='blue')\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    xgboost, X_test, y_test,\n",
    "    color='skyblue', ax=disp.ax_)\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    lgbm, X_train, y_train,\n",
    "    color='red', ax=disp.ax_)\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    lgbm, X_test, y_test,\n",
    "    color='pink', ax=disp.ax_)\n",
    "\"\"\"\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    lr, X_train, y_train,\n",
    "    color='green', ax=disp.ax_)\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    lr, X_test, y_test,\n",
    "    color='lime', ax=disp.ax_)\n",
    "\"\"\"\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    dummy, X_test, y_test,\n",
    "    color=\"tab:orange\", linestyle=\"--\", ax=disp.ax_)\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\\n(also known as sensitivity or recall)\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = disp.ax_.set_title(\"Receiver Operating Characteristic curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75851098",
   "metadata": {},
   "source": [
    "Les évaluations donnent plusieurs pistes. Le LightGBM est globalement le modèle le plus performant. Cependant, sa bonne tenue est plus difficile à conserver (remarque empirique). De ce fait, XGBoost sera conservé comme modèle final. Ses performances sont correctes, tout en maintenant une excellente constance au fil des différents tests menés. Il propose en outre un bon support des logiciels tiers (exemples : Plotly et SHAP), ce qui permet d'envisager des représentations graphiques pertinentes et impactantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1621e8",
   "metadata": {},
   "source": [
    "### Fine Tuning du modèle sélectionné<a class=\"anchor\" id=\"section_3_8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'espace de recherche du XGBoost pour la BayesSearchCV\n",
    "ftxgboost_space = dict()\n",
    "ftxgboost_space['ftxgb__n_estimators'] = np.linspace(100, 3000, 1000, dtype=int).tolist()\n",
    "ftxgboost_space['ftxgb__gamma'] = [1, 2]\n",
    "ftxgboost_space['ftxgb__colsample_bytree'] = np.linspace(0.8, 1.0, num=10).tolist()\n",
    "ftxgboost_space['ftxgb__subsample'] = [1]\n",
    "ftxgboost_space['ftxgb__max_depth'] = [2, 3]\n",
    "ftxgboost_space['ftxgb__reg_alpha'] = [0.02, 0.03, 0.04]\n",
    "ftxgboost_space['ftxgb__reg_lambda'] = [0.05, 0.06, 0.07]\n",
    "ftxgboost_space['ftxgb__min_child_weight'] = np.linspace(800, 1200, 100, dtype=int).tolist()\n",
    "ftxgboost_space['ftxgb__tree_method'] = ['gpu_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95997da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    ftxgboost = XGBClassifier()\n",
    "\n",
    "    ftxgboost_pipeline = imbpipeline([('smote', SMOTE(random_state=42)), ('ftxgb', ftxgboost)])\n",
    "\n",
    "    ftxgboost_grid = BayesSearchCV(estimator=ftxgboost_pipeline,\n",
    "                                     search_spaces=ftxgboost_space,\n",
    "                                     n_jobs=-1,\n",
    "                                     cv=cv,\n",
    "                                     scoring=scoring,\n",
    "                                     refit='fbeta_loss',\n",
    "                                     n_iter=30)\n",
    "    ftxgboost_grid.fit(X_train, y_train);\n",
    "    \n",
    "    ftxgboost = XGBClassifier(tree_method=ftxgboost_grid.best_params_['ftxgb__tree_method'],\n",
    "                              subsample=ftxgboost_grid.best_params_['ftxgb__subsample'],\n",
    "                              reg_lambda=ftxgboost_grid.best_params_['ftxgb__reg_lambda'],\n",
    "                              reg_alpha=ftxgboost_grid.best_params_['ftxgb__reg_alpha'],\n",
    "                              min_child_weight=ftxgboost_grid.best_params_['ftxgb__min_child_weight'],\n",
    "                              max_depth=ftxgboost_grid.best_params_['ftxgb__max_depth'],\n",
    "                              gamma=ftxgboost_grid.best_params_['ftxgb__gamma'],\n",
    "                              colsample_bytree=ftxgboost_grid.best_params_['ftxgb__colsample_bytree'],\n",
    "                              predictor='gpu_predictor')\n",
    "    \n",
    "    with timer(\"Entraînement du Fine Tuned XGBoost model\"):\n",
    "        ftxgboost.fit(X_train, y_train);\n",
    "    \n",
    "    generate_stats(ftxgboost, 'fine_tuned_XGBoost', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecfd43",
   "metadata": {},
   "source": [
    "### Évaluation de la courbe ROC (AUC) du modèle Fine Tuned<a class=\"anchor\" id=\"section_3_9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7acea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    xgboost, X_train, y_train, color='blue')\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    xgboost, X_test, y_test,\n",
    "    color='skyblue', ax=disp.ax_)\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    ftxgboost, X_train, y_train,\n",
    "    color='green', ax=disp.ax_)\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    ftxgboost, X_test, y_test,\n",
    "    color='lime', ax=disp.ax_)\n",
    "\n",
    "disp = RocCurveDisplay.from_estimator(\n",
    "    dummy, X_test, y_test,\n",
    "    color=\"tab:orange\", linestyle=\"--\", ax=disp.ax_)\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\\n(also known as sensitivity or recall)\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = disp.ax_.set_title(\"Receiver Operating Characteristic curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01211f99",
   "metadata": {},
   "source": [
    "### Déclaration du modèle final<a class=\"anchor\" id=\"section_3_10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3423050",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = XGBClassifier(tree_method=ftxgboost_grid.best_params_['ftxgb__tree_method'],\n",
    "                          subsample=ftxgboost_grid.best_params_['ftxgb__subsample'],\n",
    "                          reg_lambda=ftxgboost_grid.best_params_['ftxgb__reg_lambda'],\n",
    "                          reg_alpha=ftxgboost_grid.best_params_['ftxgb__reg_alpha'],\n",
    "                          min_child_weight=ftxgboost_grid.best_params_['ftxgb__min_child_weight'],\n",
    "                          max_depth=ftxgboost_grid.best_params_['ftxgb__max_depth'],\n",
    "                          gamma=ftxgboost_grid.best_params_['ftxgb__gamma'],\n",
    "                          colsample_bytree=ftxgboost_grid.best_params_['ftxgb__colsample_bytree'])\n",
    "\n",
    "with timer(\"Entraînement du Fine Tuned XGBoost model\"):\n",
    "    final_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a2daf",
   "metadata": {},
   "source": [
    "### Création des SHAP Explainer / Values pour l'interprétabilité locale et globale<a class=\"anchor\" id=\"section_3_11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.TreeExplainer(final_model, X_test, algorithm = 'auto', n_jobs = -1)\n",
    "shap_values = shap_explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c81ab1",
   "metadata": {},
   "source": [
    "### Sérialisation des modèles et des SHAP Explainer / Values<a class=\"anchor\" id=\"section_3_12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dummy, open(PICKLED_MODELS_PATH + 'dummy.pkl', 'wb'))\n",
    "pickle.dump(lr, open(PICKLED_MODELS_PATH + 'lr.pkl', 'wb'))\n",
    "pickle.dump(xgboost, open(PICKLED_MODELS_PATH + 'xgboost.pkl', 'wb'))\n",
    "pickle.dump(lgbm, open(PICKLED_MODELS_PATH + 'lightgbm.pkl', 'wb'))\n",
    "pickle.dump(ftxgboost, open(PICKLED_MODELS_PATH + 'final_model.pkl', 'wb'))\n",
    "pickle.dump(shap_explainer, open(PICKLED_MODELS_PATH + 'final_model_shap_explainer.pkl', 'wb'))\n",
    "pickle.dump(shap_values, open(PICKLED_MODELS_PATH + 'final_model_shap_values.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ce5a5",
   "metadata": {},
   "source": [
    "### Chargement des datasets sauvegardés précédemment et définition des datasets évalués<a class=\"anchor\" id=\"section_3_13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a336c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(OUTPUT_DATASETS_PATH + 'train_data.csv')\n",
    "test_df = pd.read_csv(OUTPUT_DATASETS_PATH + 'test_data.csv')\n",
    "score_df = pd.read_csv(OUTPUT_DATASETS_PATH + 'score_data.csv')\n",
    "newclients_df = pd.read_csv(OUTPUT_DATASETS_PATH + 'newclients_data.csv')\n",
    "\n",
    "dd_features = [f for f in train_df.columns if f not in ['level_0', 'AMT_CREDIT_TO_ANNUITY_RATIO', 'AMT_GOODS_PRICE_TO_ANNUITY_RATIO', 'AMT_CREDIT_GOODS_PRICE_DIFF', 'GOODS_PRICE_FAIL_RATIO', 'CREDIT_FAIL_RATIO', 'TARGET', 'SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "\n",
    "train_df = train_df[dd_features]\n",
    "test_df = test_df[dd_features]\n",
    "score_df = score_df[dd_features]\n",
    "newclients_df = newclients_df[dd_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b34801",
   "metadata": {},
   "source": [
    "# Analyse du Data Drift (evidently)<a class=\"anchor\" id=\"partie4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3474b",
   "metadata": {},
   "source": [
    "### Train vs Test<a class=\"anchor\" id=\"section_4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62109f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[DataDriftPreset(),])\n",
    "\n",
    "tests = TestSuite(tests=[\n",
    "    TestNumberOfColumnsWithMissingValues(),\n",
    "    TestNumberOfRowsWithMissingValues(),\n",
    "    TestNumberOfConstantColumns(),\n",
    "    TestNumberOfDuplicatedRows(),\n",
    "    TestNumberOfDuplicatedColumns(),\n",
    "    TestColumnsType(),\n",
    "    TestNumberOfDriftedColumns(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=train_df, current_data=test_df)\n",
    "report.save_html(EVIDENTLY_PATH + 'report_train_test.html')\n",
    "\n",
    "tests.run(reference_data=train_df, current_data=test_df)\n",
    "tests.save_html(EVIDENTLY_PATH + 'stability_train_test.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb617871",
   "metadata": {},
   "source": [
    "### Train vs Score<a class=\"anchor\" id=\"section_4_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06320575",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[DataDriftPreset(),])\n",
    "\n",
    "tests = TestSuite(tests=[\n",
    "    TestNumberOfColumnsWithMissingValues(),\n",
    "    TestNumberOfRowsWithMissingValues(),\n",
    "    TestNumberOfConstantColumns(),\n",
    "    TestNumberOfDuplicatedRows(),\n",
    "    TestNumberOfDuplicatedColumns(),\n",
    "    TestColumnsType(),\n",
    "    TestNumberOfDriftedColumns(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=train_df, current_data=score_df)\n",
    "report.save_html(EVIDENTLY_PATH + 'report_train_score.html')\n",
    "\n",
    "tests.run(reference_data=train_df, current_data=score_df)\n",
    "tests.save_html(EVIDENTLY_PATH + 'stability_train_score.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6157ea",
   "metadata": {},
   "source": [
    "### Train vs App<a class=\"anchor\" id=\"section_4_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[DataDriftPreset(),])\n",
    "\n",
    "tests = TestSuite(tests=[\n",
    "    TestNumberOfColumnsWithMissingValues(),\n",
    "    TestNumberOfRowsWithMissingValues(),\n",
    "    TestNumberOfConstantColumns(),\n",
    "    TestNumberOfDuplicatedRows(),\n",
    "    TestNumberOfDuplicatedColumns(),\n",
    "    TestColumnsType(),\n",
    "    TestNumberOfDriftedColumns(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=train_df, current_data=newclients_df)\n",
    "report.save_html(EVIDENTLY_PATH + 'report_train_app.html')\n",
    "\n",
    "tests.run(reference_data=train_df, current_data=newclients_df)\n",
    "tests.save_html(EVIDENTLY_PATH + 'stability_train_app.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
